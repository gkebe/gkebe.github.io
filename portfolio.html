<!DOCTYPE html>
<html>
    
    <head>
        <meta charset="utf-8"/>
        <title>Gaoussou Youssouf Kebe</title>
        <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
        
        <!-- Font Awesome Icons -->
        <script src="https://kit.fontawesome.com/cb661d545f.js" crossorigin="anonymous"></script>

        
        <!-- Bootstrap -->
        <link href="css/bootstrap.min.css" rel="stylesheet"/>
        <!--<link href="css/bootstrap.min.css" rel="stylesheet">-->
        
        <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
        <!--[if lt IE 9]>
         <script src="js/html5shiv.js"></script>
         <script src="js/respond.min.js"></script>
         <![endif]-->
        
        
        <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
        <!-- http://getbootstrap.com/customize/ change "@screen-sm" -> "@screen-md" in @grid-float-breakpoint -->
        <script src="js/jquery.min.js"></script>
        <!-- Global site tag (gtag.js) - Google Analytics -->
        
<script async src="https://www.googletagmanager.com/gtag/js?id=G-85G9XGQE96"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-85G9XGQE96');
</script>
        <!-- Include all compiled plugins (below), or include individual files as needed -->
        <script src="js/bootstrap.min.js"></script>
        <script src="js/menucollapse.js"></script>
        <script type="text/javascript" src="js/arrow78.js"></script>
        <script type="text/javascript" src="js/custom.js"></script>
        <style>
        .nav>li>a {
   			font-size: 21px;
		}
		/*
  AUTO GRID
  Set the minimum item size with `--auto-grid-min-size` and you'll
  get a fully responsive grid with no media queries.
*/
.auto-grid {
  --auto-grid-min-size: 16rem;
}

.auto-grid > * {
  max-width: 400px;
}

.auto-grid > * + * {
  margin-top: 1rem;
}

@supports(display: grid) {
  .auto-grid {
  padding: 0px 1rem;
  margin-top: 30px;
    display: grid;
    grid-template-columns: repeat(auto-fill, minmax(400px, 1fr));
    grid-gap: 1rem;
  }
  
  .auto-grid > * {
    max-width: unset;
  }

  .auto-grid > * + * {
    margin-top: unset;
  }
  @media(min-width:1800px){
  grid-template-columns: repeat(auto-fill, minmax(600px, 1fr));
  }
}

/*
  WRAPPER
  A utility with a max width that contains child elements and horizontal centers them
*/
.wrapper {
  max-width: 65rem;
  margin: 0 auto;
  padding: 0 1rem;
}


/* Presentational styles */

body {
  background: #e7e7e7;
  padding: 1rem 0;
  line-height: 1.4;
  font-family: sans-serif;
}

#portfolio li {
  text-align: center;
  font-size: 1.2rem;
  background: #fff;
  color: #000;
  list-style: none;
}
		.project {
    cursor: pointer;
    padding: 8px;
    text-align: center;
    
    padding-bottom: 20px;
}
.project_details{
    margin-top: 10px;
    padding: 0px 30px;
    padding-top: 0px;
    display: inline-block;
    font-size: 18px;
}
.project_links{
margin-top:10px
}
.project_links a{
margin:5px;
}
.no_links{
text-align:center;
font-style: italic;
    color: #a0a0a0;
}
.project .project_details{
display:none;
}

.project_image {
    padding: 15px 0px 0px;
    width: 100%;
    float: left;
  }

.project_text {
    padding: 0px 30px;
    padding-top: 0px;
    display: inline-block;
}
.project_text h3{
font-family: inherit;
    font-weight: 700;
    font-style: inherit;
    font-size: 22px;
    line-height: 1.14;
    -moz-transition-property: color;
    -o-transition-property: color;
    -webkit-transition-property: color;
    transition-property: color;
    -moz-transition-duration: .3s;
    -o-transition-duration: .3s;
    -webkit-transition-duration: .3s;
    transition-duration: .3s;
}
.project_text p{
-moz-transition-property: color;
    -o-transition-property: color;
    -webkit-transition-property: color;
    transition-property: color;
    -moz-transition-duration: .3s;
    -o-transition-duration: .3s;
    -webkit-transition-duration: .3s;
    transition-duration: .3s;
    margin: 5px 0 0;
    font-size: 18px;
    color: #a0a0a0;
}
.project .project_text p{
    overflow: hidden;
text-overflow: ellipsis;
-webkit-line-clamp: 3;
display: -webkit-box;
-webkit-box-orient: vertical;}
.profile {
    width: 650px;
}

.project:hover h3, .project:hover p{
color:#00aeef;}
@media (min-width: 1024px){
.modal-dialog {
    width: 1024px;
}
.modal-dialog .project_image{
    width: 70%;
    float: left;
    margin-left: 15%;
}
}
        </style>
    </head>
    
    <body id="page-top" class="index">
        
        <!-- Navigation -->
        <nav class="navbar navbar-default navbar-fixed-top">
            <div class="container-fluid">
                <!-- Brand and toggle get grouped for better mobile display -->
                <div class="navbar-header">
                    <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"  data-target="#bs-example-navbar-collapse-2" aria-expanded="false">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="glyphicon glyphicon-search"></span>
                    </button>
                    <button id="button2" type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                </div>
                <!-- Collect the nav links, forms, and other content for toggling -->
                <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                    <ul class="nav navbar-nav navbar-right">
                        <li>
                            <a href="/">Home</a>
                        </li>
                        <li class="active">
                            <a href="#">Portfolio</a>
                        </li>
                    </ul>
                </div><!-- /.navbar-collapse -->
                
                <!-- search box submenu -->
                <!--<div class="collapse" id="bs-example-navbar-collapse-2">
                    <gcse:search></gcse:search>
                        </div>
                
            </div>--><!-- /.container-fluid -->
        </nav> 
        
        
        <section>--> 
            <!-- Place this tag where you want the search results to render -->
            <!--<gcse:searchresults-only></gcse:searchresults-only>
                </section>
    --> 
        
        <!-- Home Section -->
        <!-- Home Section -->
        <section name="portfolio" id="portfolio">
        <ul class="auto-grid">
          <li><div class="project">
  <img class="project_image" src="vr.png"/>
  <div class="project_text">
    <h3>Speech Dataset for Action Labeling and Bias Mitigation in HRI</h3>
    <p>A speech-based Grounded Language Learning dataset that combines scenarios of a robot doing tasks and spoken natural language utterances.</p>
</div>
  <div class="project_details">
  <b>Abstract:</b> We are working on collecting a speech-based Grounded Language Learning dataset that combines visual scenarios of a robot doing tasks and natural language utterances. The visual data will be from a simulated environment. The language data will include low-level commands, high level descriptions of the robotic task and contextual explanations of the human intention behind the robot's actions. We intend to collect this data in the form of audio recordings through Amazon Mechanical Turk (AMT).
  <div class="project_links no_links">
  Work in progress
  </div>
  </div>
</div></li>
  <li><div class="project">
  <img class="project_image" src="speech_grounding.png"/>
  <div class="project_text">
    <h3>Grounding Spoken Language to Robotic Perception</h3>
    <p>A grounded language acquisition approach that learns directly from end-user speech, without relying on intermediate textual representations. This will allow interactions in which language about novel tasks and environments is learned from end users, reducing dependence on textual inputs and potentially mitigating the effects of demographic bias found in widely available speech recognition systems.</p>
  </div>
  <div class="project_details">
  <b>Abstract:</b> Learning to understand grounded language, which connects natural language to percepts, is a critical research area. Prior work in grounded language acquisition has focused primarily on textual inputs. In this work we demonstrate the feasibility of performing grounded language acquisition on paired visual percepts and raw speech inputs. This will allow interactions in which language about novel tasks and environments is learned from end users, reducing dependence on textual inputs and potentially mitigating the effects of demographic bias found in widely available speech recognition systems. We leverage recent work in self-supervised speech representation models and show that learned representations of speech can make language grounding systems more inclusive towards specific groups while maintaining or even increasing general performance.

    <div class="project_links no_links">
  Accepted to AAAI 2022. Link to paper coming soon.
    </div>
  </div>
</div></li>
  <li><div class="project">
  <img class="project_image" src="gold.png"/>
  <div class="project_text">
    <h3>GoLD: A Spoken Language Grounding Dataset</h3>
    <p>The Grounded Language Dataset, or GoLD, is a grounded language learning dataset in four modalities: RGB, depth, text, speech. The data contains 207 instances of 47 object classes. The objects are from five high level categories of food, home, medical, office, and tool. Each instance is captured from different angles for a total of 825 images. Text and speech descriptions are collected using Amazon Mechanical Turk (AMT) for a total of 16500 text descriptions and 16500 speech descriptions.</p>
    </div>
      <div class="project_details">
      <b>Abstract:</b> Grounded language acquisition is a major area of research combining aspects of natural language processing, computer vision, and signal processing, compounded by domain issues requiring sample efficiency and other deployment constraints. In this work, we present a multimodal dataset of RGB+depth objects with spoken as well as textual descriptions. We analyze the differences between the two types of descriptive language and our experiments demonstrate that the different modalities affect learning. This will enable researchers studying the intersection of robotics, NLP, and HCI to better investigate how the multiple modalities of image, depth, text, speech, and transcription interact, as well as how differences in the vernacular of these modalities impact results.
  
      <div class="project_links">
    <a style="display: inline-block" target="_blank" href="https://openreview.net/pdf?id=Yx9jT3fkBaD">
                  <i class="fa fa-file-pdf"></i> NeurIPS 2021 Paper
    </a>
        <a style="display: inline-block" target="_blank" href="https://github.com/iral-lab/gold">
                  <i class="fa fa-github"></i> Dataset
    </a>
    </div>
  </div>
</div></li>
  <li><div class="project">
  <img class="project_image" src="manifold_alignment.png"/>
  <div class="project_text">
    <h3>Aligning Language and Robotic Perception</h3>
    <p>A cross-modality manifold alignment procedure that leverages triplet loss to jointly learn consistent, multi-modal embeddings of language-based concepts of real-world items.</p>
</div>
  <div class="project_details">
  <b>Abstract:</b> We propose a cross-modality manifold alignment procedure that leverages triplet loss to jointly learn consistent, multi-modal embeddings of language-based concepts of real-world items. Our approach learns these embeddings by sampling triples of anchor, positive, and negative data points from RGB-depth images and their natural language descriptions. We show that our approach can benefit from, but does not require, post-processing steps such as Procrustes analysis, in contrast to some of our baselines which require it for reasonable performance. We demonstrate the effectiveness of our approach on two datasets commonly used to develop robotic-based grounded language learning systems, where our approach outperforms four baselines, including a state-of-the-art approach, across five evaluation metrics.
        <div class="project_links">
    <a style="display: inline-block" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2021W/MULA/papers/Nguyen_Practical_Cross-Modal_Manifold_Alignment_for_Robotic_Grounded_Language_Learning_CVPRW_2021_paper.pdf">
                  <i class="fa fa-file-pdf"></i> MULA 2021 Paper
    </a>
        <a style="display: inline-block" target="_blank" href="https://github.com/gkebe/manifold_alignment/tree/gold">
                  <i class="fa fa-github"></i> Code
    </a>
    </div>
  </div>
</div></li>
  <li><div class="project">
  <img class="project_image" src="eventbert.png"/>
  <div class="project_text">
    <h3>EventBERT: Pre-training Approach for Event Modeling</h3>
    <p>An BERT-based event modeling approach where Masked Language Modeling (MLM)  and  Next Sentence Prediction (NSP) objectives are trained on a large amount of structured event documents.</p>
</div>
  <div class="project_details">
  <b>Abstract:</b> Pre-training language representation models on large-scale corpora has led to state-of-the-art performance in multiple NLP tasks. These models are particularly efficient at transferring the knowledge acquired from their pre-training objectives to other tasks. In this paper, we apply one of these models in BERT, to the context of event modeling. We show that pre-training BERT with its Masked Language Modeling (MLM) and Next Sentence Prediction (NSP) objectives on a large amount of structured event documents can lead to state-of-the-art performance in event modeling tasks.
  <img src="EventBERT_plot.png">
  <div class="project_links">

        <a style="display: inline-block" target="_blank" href="https://github.com/gkebe/EventBERT">
                  <i class="fa fa-github"></i> Code
    </a>
    </div>
  </div>
</div></li>
  <li><div class="project">
  <img class="project_image" src="emotions.png"/>
  <div class="project_text">
    <h3>BERT vs. XLNet for Multi-label Emotion Analysis</h3>
    <p>A comparative analysis of pre-trained self-supervised models BERT and XLNet on a multilabel emotion analysis task.</p>
</div>
  <div class="project_details">
  <b>Abstract:</b> The complex nature of emotions makes it also one of the hardest text classification tasks. We present a comparative study of state of the art language representation models XLNet and Bert in sentiment analysis, specifically multi-label classification of tweets among 6 basic emotions (anger, disgust, fear, joy, sadness and surprise).
  <div class="project_links">
    <a style="display: inline-block" target="_blank" href="bert_xlnet.pdf">
                  <i class="fa fa-file-pdf"></i> MASC-SLL 2020 Abstract
    </a>
        <a style="display: inline-block" target="_blank" href="https://github.com/gkebe/twitter-emotion-analysis">
                  <i class="fa fa-github"></i> Code
    </a>
    </div>
  </div>
</div></li>
</ul>

        </section>

        
        <hr class="star-primary">
        <footer>
            <small>
                <center>
                    Credits: AR template
                    <div style="display:none;" id="credit">[AR template available under Creative Commons CC BY 4.0 licence:
                        <a href="https://github.com/dmsl/academic-responsive-template" target="_blank">
                            https://github.com/dmsl/academic-responsive-template
                        </a> ]
                    </div>
                </center>
            </small>
        </footer>
        <div class="modal fade" id="projectmodal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
                <h4 class="modal-title" id="myModalLabel"></h4>
            </div>
            <div class="modal-body">
                
            </div>
            <div class="modal-footer">
                <button type="button" class="btn btn-default" data-dismiss="modal">Close</button>
            </div>
        </div>
    </div>
</div>
        <script>
$(document).ready(function() {
    $(function() {
        $(".project").hover(
            function() {
                image_name = $(this).find(".project_image").attr('src');
                image_name = image_name.replace('png', 'gif');
                $(this).find(".project_image").attr("src", image_name);
            },
            function() {
                image_name = $(this).find(".project_image").attr('src');
                image_name = image_name.replace('gif', 'png');
                $(this).find(".project_image").attr("src", image_name);
            }                         
        );                  
    });
    
    $(document).ready(function () {

     // Attach Button click event listener 
    $(".project").click(function(){
         // show Modal
         $('#projectmodal').modal('show');
         $('#projectmodal').find(".modal-body").html($(this).html());
         $('#projectmodal').find(".project_image").addClass("col-md-6");
		 image_name = $('#projectmodal').find(".project_image").attr('src');
         image_name = image_name.replace('gif', 'png');
         $('#projectmodal').find(".project_image").attr("src", image_name);
    });
});
    });
</script>
    </body>
</html>
